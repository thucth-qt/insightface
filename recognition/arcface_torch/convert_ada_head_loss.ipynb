{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def l2_norm(input,axis=-1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "class AdaFaceWAct(torch.nn.Module):\n",
    "    ''' \n",
    "    1. Multiply embeddings with W (W phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 embedding_size=512,\n",
    "                 classnum=70722,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0,\n",
    "                 ):\n",
    "        super(AdaFaceWAct, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = torch.nn.Parameter(torch.Tensor(embedding_size,classnum))\n",
    "\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = m \n",
    "        self.eps = 1e-3\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "\n",
    "        # ema prep\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('t', torch.zeros(1))\n",
    "        self.register_buffer('batch_mean', torch.ones(1)*(20))\n",
    "        self.register_buffer('batch_std', torch.ones(1)*100)\n",
    "\n",
    "        print('\\n\\AdaFaceWAct with the following property')\n",
    "        print('self.m', self.m)\n",
    "        print('self.h', self.h)\n",
    "        print('self.s', self.s)\n",
    "        print('self.t_alpha', self.t_alpha)\n",
    "\n",
    "    def forward(self, embbedings, norms, label):\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        cosine = torch.mm(embbedings,kernel_norm)\n",
    "        cosine = cosine.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "\n",
    "        safe_norms = torch.clip(norms, min=0.001, max=100) # for stability\n",
    "        safe_norms = safe_norms.clone().detach()\n",
    "\n",
    "        # update batchmean batchstd\n",
    "        with torch.no_grad():\n",
    "            mean = safe_norms.mean().detach()\n",
    "            std = safe_norms.std().detach()\n",
    "            self.batch_mean = mean * self.t_alpha + (1 - self.t_alpha) * self.batch_mean\n",
    "            self.batch_std =  std * self.t_alpha + (1 - self.t_alpha) * self.batch_std\n",
    "\n",
    "        margin_scaler = (safe_norms - self.batch_mean) / (self.batch_std+self.eps) # 66% between -1, 1\n",
    "        margin_scaler = margin_scaler * self.h # 68% between -0.333 ,0.333 when h:0.333\n",
    "        margin_scaler = torch.clip(margin_scaler, -1, 1)\n",
    "        # ex: m=0.5, h:0.333\n",
    "        # range\n",
    "        #       (66% range)\n",
    "        #   -1 -0.333  0.333   1  (margin_scaler)\n",
    "        # -0.5 -0.166  0.166 0.5  (m * margin_scaler)\n",
    "\n",
    "        # g_angular\n",
    "        m_arc = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
    "        m_arc.scatter_(1, label.reshape(-1, 1), 1.0)\n",
    "        g_angular = self.m * margin_scaler * -1\n",
    "        m_arc = m_arc * g_angular\n",
    "        theta = cosine.acos()\n",
    "        theta_m = torch.clip(theta + m_arc, min=self.eps, max=math.pi-self.eps)\n",
    "        cosine = theta_m.cos()\n",
    "\n",
    "        # g_additive\n",
    "        m_cos = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
    "        m_cos.scatter_(1, label.reshape(-1, 1), 1.0)\n",
    "        g_add = self.m + (self.m * margin_scaler)\n",
    "        m_cos = m_cos * g_add\n",
    "        cosine = cosine - m_cos\n",
    "        # scale\n",
    "        scaled_cosine_m = cosine * self.s\n",
    "        return scaled_cosine_m\n",
    "\n",
    "cross_entropy_loss = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\AdaFaceWAct with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 1.0\n"
     ]
    }
   ],
   "source": [
    "adaface_w_act = AdaFaceWAct(embedding_size=512,\n",
    "                 classnum=10,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs.shape:  torch.Size([2, 512])\n",
      "norm.shape:  torch.Size([2, 1])\n",
      "labels.shape:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input/labels\n",
    "embs = torch.randn(2,512)\n",
    "norm = torch.norm(embs,2,dim=-1,keepdim=True)\n",
    "embs = torch.div(embs, norm)\n",
    "labels = torch.tensor([5,9])\n",
    "print(\"embs.shape: \",embs.shape) \n",
    "print(\"norm.shape: \",norm.shape) \n",
    "print(\"labels.shape: \",labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = adaface_w_act(embs,norm,labels)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor(25.2385, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "loss_train = cross_entropy_loss(logits, labels)\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate W and Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def l2_norm(input,axis=-1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "    \n",
    "class AdaFC(torch.nn.Module):\n",
    "    ''' \n",
    "    1. Multiply embeddings with W (FC phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 margin_loss: Callable,\n",
    "                 embedding_size=512,\n",
    "                 classnum=70722,\n",
    "                 ):\n",
    "        super(AdaFC, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = torch.nn.Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        self.dist_cross_entropy = CrossEntropyLoss()\n",
    "\n",
    "        # margin_loss\n",
    "        if isinstance(margin_loss, Callable):\n",
    "            self.margin_softmax = margin_loss\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.eps = 1e-3\n",
    "\n",
    "    def forward(self, embbedings, norms, label):\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        logits = torch.mm(embbedings,kernel_norm)\n",
    "        logits = logits.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "\n",
    "        logits = self.margin_softmax(logits,norms, labels)\n",
    "        loss = self.dist_cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "class AdaAct(torch.nn.Module):\n",
    "    ''' \n",
    "    1. Multiply embeddings with W (FC phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0,\n",
    "                 ):\n",
    "        super(AdaAct, self).__init__()\n",
    "        self.m = m \n",
    "        self.eps = 1e-3\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "        self.theta = math.cos(math.pi - m)\n",
    "        \n",
    "        self.easy_margin = False\n",
    "        # ema prep\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('batch_mean_z', torch.ones(1)*(20))\n",
    "        self.register_buffer('batch_std_z', torch.ones(1)*100)\n",
    "\n",
    "        print('\\n\\AdaFaceWAct with the following property')\n",
    "        print('self.m', self.m)\n",
    "        print('self.h', self.h)\n",
    "        print('self.s', self.s)\n",
    "        print('self.t_alpha', self.t_alpha)\n",
    "\n",
    "    def forward(self, logits:torch.Tensor, norms:torch.Tensor, labels:torch.Tensor):\n",
    "        logits = logits.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "        \n",
    "        safe_norms = torch.clip(norms, min=0.001, max=100) # for stability\n",
    "        safe_norms = safe_norms.clone().detach()\n",
    "\n",
    "        # update batchmean batchstd\n",
    "        with torch.no_grad():\n",
    "            mean_z = safe_norms.mean().detach()\n",
    "            std_z = safe_norms.std().detach()\n",
    "            self.batch_mean_z = mean_z * self.t_alpha + (1 - self.t_alpha) * self.batch_mean_z\n",
    "            self.batch_std_z =  std_z * self.t_alpha + (1 - self.t_alpha) * self.batch_std_z\n",
    "\n",
    "\n",
    "        z = (safe_norms - self.batch_mean_z) / (self.batch_std_z+self.eps)\n",
    "        z = z * self.h \n",
    "        z = torch.clip(z, -1, 1)\n",
    "\n",
    "        # g_angular shape(2,1)\n",
    "        g_angular = - self.m * z \n",
    "        g_angular = g_angular.reshape(-1)\n",
    "        \n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logits = logits[index, labels[index].view(-1)]\n",
    "\n",
    "        theta = target_logits.acos()\n",
    "        theta_m = torch.clip(theta + g_angular, min=self.eps, max=math.pi-self.eps)######\n",
    "        target_logits_angular = theta_m.cos()\n",
    "\n",
    "        # g_additive sahpe(2,1)\n",
    "        g_add = self.m + (self.m * z)\n",
    "        g_add = g_add.reshape(-1)\n",
    "        target_logits_add = target_logits_angular - g_add\n",
    "        # this is not easy_marin in arcface\n",
    "        gap_ = 1 - self.m*z - self.m - (self.m*z).cos()\n",
    "        gap_ = gap_.reshape(-1)\n",
    "\n",
    "        final_target_logits = torch.where(theta + g_angular > 0, target_logits_add, target_logits+gap_)\n",
    "\n",
    "        logits[index, labels[index].view(-1)] = final_target_logits\n",
    "        logits = logits * self.s\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\AdaFaceWAct with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 1.0\n"
     ]
    }
   ],
   "source": [
    "ada_act = AdaAct(m=0.4, h=0.333, s=64., t_alpha=1.0)\n",
    "ada_fc = AdaFC(margin_loss=ada_act, embedding_size=512, classnum=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.5665, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ada_fc(embs,norm,labels)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch19",
   "language": "python",
   "name": "torch19"
  },
  "language_info": {
   "name": "",
   "version": ""
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
