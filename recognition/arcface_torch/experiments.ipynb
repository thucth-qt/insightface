{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thucth/thucth/project/insightface/recognition/arcface_torch\n"
     ]
    }
   ],
   "source": [
    "%cd /home/thucth/thucth/project/insightface/recognition/arcface_torch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Arcface into AdaFace heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs.shape:  torch.Size([2, 512])\n",
      "norm.shape:  torch.Size([2, 1])\n",
      "labels.shape:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input/labels\n",
    "embs = torch.randn(2,512)\n",
    "norm = torch.norm(embs,2,dim=-1,keepdim=True)\n",
    "embs = torch.div(embs, norm)\n",
    "labels = torch.tensor([6,9])\n",
    "print(\"embs.shape: \",embs.shape) \n",
    "print(\"norm.shape: \",norm.shape) \n",
    "print(\"labels.shape: \",labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\AdaFaceWAct with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 1.0\n",
      "tensor([[0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442],\n",
      "        [0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442],\n",
      "        [0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442],\n",
      "        ...,\n",
      "        [0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442],\n",
      "        [0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442],\n",
      "        [0.0442, 0.0442, 0.0442,  ..., 0.0442, 0.0442, 0.0442]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[ 0.0216,  0.0216,  0.0216,  0.0216,  0.0216,  0.0216,  0.0216,  0.0216,\n",
      "          0.0216,  0.0216],\n",
      "        [-0.0279, -0.0279, -0.0279, -0.0279, -0.0279, -0.0279, -0.0279, -0.0279,\n",
      "         -0.0279, -0.0279]], grad_fn=<ClampBackward1>)\n",
      "tensor(27.7968, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def l2_norm(input,axis=-1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "class AdaFaceWAct(torch.nn.Module):\n",
    "    ''' \n",
    "    1. Multiply embeddings with W (W phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 embedding_size=512,\n",
    "                 classnum=70722,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0,\n",
    "                 ):\n",
    "        super(AdaFaceWAct, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = torch.nn.Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        # # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        # self.kernel = torch.nn.Parameter(torch.ones(embedding_size,classnum)) for debug without randomness\n",
    "\n",
    "        self.m = m \n",
    "        self.eps = 1e-3\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "\n",
    "        # ema prep\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('t', torch.zeros(1))\n",
    "        self.register_buffer('batch_mean', torch.ones(1)*(20))\n",
    "        self.register_buffer('batch_std', torch.ones(1)*100)\n",
    "\n",
    "        print('\\n\\AdaFaceWAct with the following property')\n",
    "        print('self.m', self.m)\n",
    "        print('self.h', self.h)\n",
    "        print('self.s', self.s)\n",
    "        print('self.t_alpha', self.t_alpha)\n",
    "\n",
    "    def forward(self, embbedings, norms, label):\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        print(kernel_norm)\n",
    "        cosine = torch.mm(embbedings,kernel_norm)\n",
    "        cosine = cosine.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "        \n",
    "        print(cosine)\n",
    "        \n",
    "        safe_norms = torch.clip(norms, min=0.001, max=100) # for stability\n",
    "        safe_norms = safe_norms.clone().detach()\n",
    "\n",
    "        # update batchmean batchstd\n",
    "        with torch.no_grad():\n",
    "            mean = safe_norms.mean().detach()\n",
    "            std = safe_norms.std().detach()\n",
    "            self.batch_mean = mean * self.t_alpha + (1 - self.t_alpha) * self.batch_mean\n",
    "            self.batch_std =  std * self.t_alpha + (1 - self.t_alpha) * self.batch_std\n",
    "\n",
    "        margin_scaler = (safe_norms - self.batch_mean) / (self.batch_std+self.eps) # 66% between -1, 1\n",
    "        margin_scaler = margin_scaler * self.h # 68% between -0.333 ,0.333 when h:0.333\n",
    "        margin_scaler = torch.clip(margin_scaler, -1, 1)\n",
    "        # ex: m=0.5, h:0.333\n",
    "        # range\n",
    "        #       (66% range)\n",
    "        #   -1 -0.333  0.333   1  (margin_scaler)\n",
    "        # -0.5 -0.166  0.166 0.5  (m * margin_scaler)\n",
    "\n",
    "        # g_angular\n",
    "        m_arc = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
    "        m_arc.scatter_(1, label.reshape(-1, 1), 1.0)\n",
    "        g_angular = self.m * margin_scaler * -1\n",
    "        m_arc = m_arc * g_angular\n",
    "        theta = cosine.acos()\n",
    "        theta_m = torch.clip(theta + m_arc, min=self.eps, max=math.pi-self.eps)\n",
    "        cosine = theta_m.cos()\n",
    "\n",
    "        # g_additive\n",
    "        m_cos = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
    "        m_cos.scatter_(1, label.reshape(-1, 1), 1.0)\n",
    "        g_add = self.m + (self.m * margin_scaler)\n",
    "        m_cos = m_cos * g_add\n",
    "        cosine = cosine - m_cos\n",
    "        # scale\n",
    "        scaled_cosine_m = cosine * self.s\n",
    "        return scaled_cosine_m\n",
    "\n",
    "cross_entropy_loss = CrossEntropyLoss()\n",
    "\n",
    "adaface_w_act = AdaFaceWAct(embedding_size=512,\n",
    "                 classnum=10,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0)\n",
    "\n",
    "logits = adaface_w_act(embs,norm,labels)\n",
    "\n",
    "loss_train = cross_entropy_loss(logits, labels)\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate W and Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def l2_norm(input,axis=-1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "    \n",
    "class AdaFC(torch.nn.Module):\n",
    "    ''' \n",
    "    1. Multiply embeddings with W (FC phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 margin_loss: Callable,\n",
    "                 embedding_size=512,\n",
    "                 classnum=70722,\n",
    "                 ):\n",
    "        super(AdaFC, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = torch.nn.Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        # self.kernel = torch.nn.Parameter(torch.ones(embedding_size,classnum)) for debug without randomness\n",
    "\n",
    "        self.dist_cross_entropy = CrossEntropyLoss()\n",
    "\n",
    "        # margin_loss\n",
    "        if isinstance(margin_loss, Callable):\n",
    "            self.margin_softmax = margin_loss\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        # initial kernel\n",
    "        self.eps = 1e-3\n",
    "\n",
    "    def forward(self, embbedings, norms, labels):\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        print(kernel_norm)\n",
    "        logits = torch.mm(embbedings,kernel_norm)\n",
    "        logits = logits.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "        print(logits)\n",
    "        logits = self.margin_softmax(logits,norms, labels)\n",
    "        loss = self.dist_cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "class AdaAct(torch.nn.Module):\n",
    "    ''' \n",
    "    This version is modified as ArcFace method\n",
    "    1. Multiply embeddings with W (FC phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0,\n",
    "                 ):\n",
    "        super(AdaAct, self).__init__()\n",
    "        self.m = m \n",
    "        self.eps = 1e-3\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "        \n",
    "        # ema prep\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('batch_mean_z', torch.ones(1)*(20))\n",
    "        self.register_buffer('batch_std_z', torch.ones(1)*100)\n",
    "\n",
    "        print('\\n\\AdaFaceWAct with the following property')\n",
    "        print('self.m', self.m)\n",
    "        print('self.h', self.h)\n",
    "        print('self.s', self.s)\n",
    "        print('self.t_alpha', self.t_alpha)\n",
    "\n",
    "    def forward(self, logits:torch.Tensor, norms:torch.Tensor, labels:torch.Tensor):\n",
    "\n",
    "        logits = logits.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "        \n",
    "        safe_norms = torch.clip(norms, min=0.001, max=100) # for stability\n",
    "        safe_norms = safe_norms.clone().detach()\n",
    "\n",
    "        # update batchmean batchstd\n",
    "        with torch.no_grad():\n",
    "            mean_z = safe_norms.mean().detach()\n",
    "            std_z = safe_norms.std().detach()\n",
    "            self.batch_mean_z = mean_z * self.t_alpha + (1 - self.t_alpha) * self.batch_mean_z\n",
    "            self.batch_std_z =  std_z * self.t_alpha + (1 - self.t_alpha) * self.batch_std_z\n",
    "\n",
    "\n",
    "        z = (safe_norms - self.batch_mean_z) / (self.batch_std_z+self.eps)\n",
    "        z = z * self.h \n",
    "        z = torch.clip(z, -1, 1)\n",
    "\n",
    "        # g_angular shape(2,1)\n",
    "        g_angular = - self.m * z \n",
    "        g_angular = g_angular.reshape(-1)\n",
    "        \n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logits = logits[index, labels[index].view(-1)]\n",
    "        print(target_logits)\n",
    "        theta = target_logits.acos()\n",
    "        theta_m = torch.clip(theta + g_angular, min=self.eps, max=math.pi-self.eps)######\n",
    "        target_logits_angular = theta_m.cos()\n",
    "\n",
    "        # g_additive sahpe(2,1)\n",
    "        g_add = self.m + (self.m * z)\n",
    "        g_add = g_add.reshape(-1)\n",
    "        target_logits_add = target_logits_angular - g_add\n",
    "        # this is not easy_marin in arcface\n",
    "        gap_ = 1 - self.m*z - self.m - (self.m*z).cos()\n",
    "        gap_ = gap_.reshape(-1)\n",
    "\n",
    "        final_target_logits = torch.where(theta + g_angular > 0, target_logits_add, target_logits+gap_)\n",
    "\n",
    "        logits[index, labels[index].view(-1)] = final_target_logits\n",
    "        logits = logits * self.s\n",
    "        return logits\n",
    "\n",
    "class AdaActOrigin(torch.nn.Module):\n",
    "    ''' \n",
    "    This version is modified as Ogirinal ArcFace \n",
    "    1. Multiply embeddings with W (FC phase)\n",
    "    2. Compute Adaface Activate (like normalized softmax) (Act phase)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 m=0.4,\n",
    "                 h=0.333,\n",
    "                 s=64.,\n",
    "                 t_alpha=1.0,\n",
    "                 ):\n",
    "        super(AdaActOrigin, self).__init__()\n",
    "        self.m = m \n",
    "        self.eps = 1e-3\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "        \n",
    "        # ema prep\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('batch_mean_z', torch.ones(1)*(20))\n",
    "        self.register_buffer('batch_std_z', torch.ones(1)*100)\n",
    "\n",
    "        print('\\n\\AdaFaceWAct with the following property')\n",
    "        print('self.m', self.m)\n",
    "        print('self.h', self.h)\n",
    "        print('self.s', self.s)\n",
    "        print('self.t_alpha', self.t_alpha)\n",
    "\n",
    "    def forward(self, logits:torch.Tensor, norms:torch.Tensor, labels:torch.Tensor):\n",
    "        logits = logits.clamp(-1+self.eps, 1-self.eps) # for stability\n",
    "        \n",
    "        safe_norms = torch.clip(norms, min=0.001, max=100) # for stability\n",
    "        safe_norms = safe_norms.clone().detach()\n",
    "\n",
    "        # update batchmean batchstd\n",
    "        with torch.no_grad():\n",
    "            mean_z = safe_norms.mean().detach()\n",
    "            std_z = safe_norms.std().detach()\n",
    "            self.batch_mean_z = mean_z * self.t_alpha + (1 - self.t_alpha) * self.batch_mean_z\n",
    "            self.batch_std_z =  std_z * self.t_alpha + (1 - self.t_alpha) * self.batch_std_z\n",
    "\n",
    "\n",
    "        margin_scaler = (safe_norms - self.batch_mean_z) / (self.batch_std_z+self.eps) # 66% between -1, 1\n",
    "        margin_scaler = margin_scaler * self.h # 68% between -0.333 ,0.333 when h:0.333\n",
    "        margin_scaler = torch.clip(margin_scaler, -1, 1)\n",
    "        # ex: m=0.5, h:0.333\n",
    "        # range\n",
    "        #       (66% range)\n",
    "        #   -1 -0.333  0.333   1  (margin_scaler)\n",
    "        # -0.5 -0.166  0.166 0.5  (m * margin_scaler)\n",
    "\n",
    "        # g_angular\n",
    "        m_arc = torch.zeros(labels.size()[0], logits.size()[1], device=logits.device)\n",
    "        m_arc.scatter_(1, labels.reshape(-1, 1), 1.0)\n",
    "        g_angular = self.m * margin_scaler * -1\n",
    "        m_arc = m_arc * g_angular\n",
    "        theta = logits.acos()\n",
    "        theta_m = torch.clip(theta + m_arc, min=self.eps, max=math.pi-self.eps)\n",
    "        logits = theta_m.cos()\n",
    "\n",
    "        # g_additive\n",
    "        m_cos = torch.zeros(labels.size()[0], logits.size()[1], device=logits.device)\n",
    "        m_cos.scatter_(1, labels.reshape(-1, 1), 1.0)\n",
    "        g_add = self.m + (self.m * margin_scaler)\n",
    "        m_cos = m_cos * g_add\n",
    "        logits = logits - m_cos\n",
    "\n",
    "        # scale\n",
    "        scaled_logits_m = logits * self.s\n",
    "        return scaled_logits_m\n",
    "\n",
    "\n",
    "ada_act = AdaAct(m=0.4, h=0.333, s=64., t_alpha=1.0)\n",
    "# ada_act = AdaActOrigin(m=0.4, h=0.333, s=64., t_alpha=1.0)\n",
    "ada_fc = AdaFC(margin_loss=ada_act, embedding_size=512, classnum=10)\n",
    "\n",
    "loss = ada_fc(embs, norm, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Ada FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    distributed.init_process_group(\"nccl\")\n",
    "except KeyError:\n",
    "    world_size = 1\n",
    "    rank = 0\n",
    "    distributed.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=\"tcp://127.0.0.1:12584\",\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch import distributed\n",
    "from torch.nn.functional import linear, normalize\n",
    "\n",
    "class AllGatherFunc(torch.autograd.Function):\n",
    "    \"\"\"AllGather op with gradient backward\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, *gather_list):\n",
    "        gather_list = list(gather_list)\n",
    "        distributed.all_gather(gather_list, tensor)\n",
    "        return tuple(gather_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        grad_list = list(grads)\n",
    "        rank = distributed.get_rank()\n",
    "        grad_out = grad_list[rank]\n",
    "\n",
    "        dist_ops = [\n",
    "            distributed.reduce(grad_out, rank, distributed.ReduceOp.SUM, async_op=True)\n",
    "            if i == rank\n",
    "            else distributed.reduce(\n",
    "                grad_list[i], i, distributed.ReduceOp.SUM, async_op=True\n",
    "            )\n",
    "            for i in range(distributed.get_world_size())\n",
    "        ]\n",
    "        for _op in dist_ops:\n",
    "            _op.wait()\n",
    "\n",
    "        grad_out *= len(grad_list)  # cooperate with distributed loss function\n",
    "        return (grad_out, *[None for _ in range(len(grad_list))])\n",
    "\n",
    "AllGather = AllGatherFunc.apply\n",
    "\n",
    "class DistCrossEntropyFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    CrossEntropy loss is calculated in parallel, allreduce denominator into single gpu and calculate softmax.\n",
    "    Implemented of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, label: torch.Tensor):\n",
    "        \"\"\" \"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        # for numerical stability\n",
    "        max_logits, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(max_logits, distributed.ReduceOp.MAX)\n",
    "        logits.sub_(max_logits)\n",
    "        logits.exp_()\n",
    "        sum_logits_exp = torch.sum(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(sum_logits_exp, distributed.ReduceOp.SUM)\n",
    "        logits.div_(sum_logits_exp)\n",
    "        index = torch.where(label != -1)[0]\n",
    "        # loss\n",
    "        loss = torch.zeros(batch_size, 1, device=logits.device)\n",
    "        loss[index] = logits[index].gather(1, label[index])\n",
    "        distributed.all_reduce(loss, distributed.ReduceOp.SUM)\n",
    "        ctx.save_for_backward(index, logits, label)\n",
    "        return loss.clamp_min_(1e-30).log_().mean() * (-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, loss_gradient):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_grad (torch.Tensor): gradient backward by last layer\n",
    "        Returns:\n",
    "            gradients for each input in forward function\n",
    "            `None` gradients for one-hot label\n",
    "        \"\"\"\n",
    "        (\n",
    "            index,\n",
    "            logits,\n",
    "            label,\n",
    "        ) = ctx.saved_tensors\n",
    "        batch_size = logits.size(0)\n",
    "        one_hot = torch.zeros(\n",
    "            size=[index.size(0), logits.size(1)], device=logits.device\n",
    "        )\n",
    "        one_hot.scatter_(1, label[index], 1)\n",
    "        logits[index] -= one_hot\n",
    "        logits.div_(batch_size)\n",
    "        return logits * loss_gradient.item(), None\n",
    "\n",
    "class DistCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, logit_part, label_part):\n",
    "        return DistCrossEntropyFunc.apply(logit_part, label_part)\n",
    "\n",
    "class AdaPartialFC(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2203.15565\n",
    "    A distributed sparsely updating variant of the FC layer, named Partial FC (PFC).\n",
    "\n",
    "    When sample rate less than 1, in each iteration, positive class centers and a random subset of\n",
    "    negative class centers are selected to compute the margin-based softmax loss, all class\n",
    "    centers are still maintained throughout the whole training process, but only a subset is\n",
    "    selected and updated in each iteration.\n",
    "\n",
    "    .. note::\n",
    "        When sample rate equal to 1, Partial FC is equal to model parallelism(default sample rate is 1).\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> module_pfc = AdaPartialFC(embedding_size=512, num_classes=8000000, sample_rate=0.2)\n",
    "    >>> for img, labels in data_loader:\n",
    "    >>>     embeddings = net(img)\n",
    "    >>>     loss = module_pfc(embeddings, labels, optimizer)\n",
    "    >>>     loss.backward()\n",
    "    >>>     optimizer.step()\n",
    "    \"\"\"\n",
    "    _version = 1 \n",
    "    def __init__(\n",
    "        self,\n",
    "        margin_loss: Callable,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        sample_rate: float = 1.0,\n",
    "        fp16: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Paramenters:\n",
    "        -----------\n",
    "        embedding_size: int\n",
    "            The dimension of embedding, required\n",
    "        num_classes: int\n",
    "            Total number of classes, required\n",
    "        sample_rate: float\n",
    "            The rate of negative centers participating in the calculation, default is 1.0.\n",
    "        \"\"\"\n",
    "        super(AdaPartialFC, self).__init__()\n",
    "        assert (\n",
    "            distributed.is_initialized()\n",
    "        ), \"must initialize distributed before create this\"\n",
    "        self.rank = distributed.get_rank()\n",
    "        self.world_size = distributed.get_world_size()\n",
    "\n",
    "        self.dist_cross_entropy = DistCrossEntropy()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sample_rate: float = sample_rate\n",
    "        self.fp16 = fp16\n",
    "        self.num_local: int = num_classes // self.world_size + int(\n",
    "            self.rank < num_classes % self.world_size\n",
    "        )\n",
    "        self.class_start: int = num_classes // self.world_size * self.rank + min(\n",
    "            self.rank, num_classes % self.world_size\n",
    "        )\n",
    "        self.num_sample: int = int(self.sample_rate * self.num_local)\n",
    "        self.last_batch_size: int = 0\n",
    "        self.weight: torch.Tensor\n",
    "        self.weight_mom: torch.Tensor\n",
    "        self.weight_activated: torch.nn.Parameter\n",
    "        self.weight_activated_mom: torch.Tensor\n",
    "        self.is_updated: bool = True\n",
    "        self.init_weight_update: bool = True\n",
    "\n",
    "        self.weight_activated = torch.nn.Parameter(torch.normal(0, 0.01, (self.num_local, embedding_size)))\n",
    "\n",
    "        # margin_loss\n",
    "        if isinstance(margin_loss, Callable):\n",
    "            self.margin_softmax = margin_loss\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        \"\"\" partial weight to global\n",
    "        \"\"\"\n",
    "        if self.init_weight_update:\n",
    "            self.init_weight_update = False\n",
    "            return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        local_embeddings: torch.Tensor,\n",
    "        norms:torch.Tensor,\n",
    "        local_labels: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        local_embeddings: torch.Tensor\n",
    "            feature embeddings on each GPU(Rank).\n",
    "        local_labels: torch.Tensor\n",
    "            labels on each GPU(Rank).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        loss: torch.Tensor\n",
    "            pass\n",
    "        \"\"\"\n",
    "\n",
    "        local_labels.squeeze_()\n",
    "        local_labels = local_labels.long()\n",
    "        self.update()\n",
    "\n",
    "        batch_size = local_embeddings.size(0)\n",
    "        if self.last_batch_size == 0:\n",
    "            self.last_batch_size = batch_size\n",
    "        assert self.last_batch_size == batch_size, (\n",
    "            \"last batch size do not equal current batch size: {} vs {}\".format(\n",
    "            self.last_batch_size, batch_size))\n",
    "\n",
    "        _gather_embeddings = [\n",
    "            torch.zeros((batch_size, self.embedding_size)).cuda()\n",
    "            for _ in range(self.world_size)\n",
    "        ]\n",
    "        _gather_labels = [\n",
    "            torch.zeros(batch_size).long().cuda() for _ in range(self.world_size)\n",
    "        ]\n",
    "        _list_embeddings = AllGather(local_embeddings, *_gather_embeddings)\n",
    "        distributed.all_gather(_gather_labels, local_labels)\n",
    "\n",
    "        embeddings = torch.cat(_list_embeddings)\n",
    "        labels = torch.cat(_gather_labels)\n",
    "\n",
    "        labels = labels.view(-1, 1)\n",
    "        index_positive = (self.class_start <= labels) & (\n",
    "            labels < self.class_start + self.num_local\n",
    "        )\n",
    "        labels[~index_positive] = -1\n",
    "        labels[index_positive] -= self.class_start\n",
    "        with torch.cuda.amp.autocast(self.fp16):\n",
    "            norm_embeddings = normalize(embeddings)\n",
    "            norm_weight_activated = normalize(self.weight_activated)\n",
    "            logits = linear(norm_embeddings, norm_weight_activated)\n",
    "        if self.fp16:\n",
    "            logits = logits.float()\n",
    "        logits = logits.clamp(-1, 1)\n",
    "        logits = self.margin_softmax(logits, norms, labels)\n",
    "        loss = self.dist_cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
    "        if destination is None: \n",
    "            destination = collections.OrderedDict()\n",
    "            destination._metadata = collections.OrderedDict()\n",
    "\n",
    "        for name, module in self._modules.items():\n",
    "            if module is not None:\n",
    "                module.state_dict(destination, prefix + name + \".\", keep_vars=keep_vars)\n",
    "        if self.sample_rate < 1:\n",
    "            destination[\"weight\"] = self.weight.detach()\n",
    "        else:\n",
    "            destination[\"weight\"] = self.weight_activated.data.detach()\n",
    "        return destination\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\AdaFaceWAct with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 1.0\n",
      "> \u001b[0;32m/tmp/ipykernel_75059/2401976465.py\u001b[0m(238)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    236 \u001b[0;31m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    237 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 238 \u001b[0;31m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    239 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    240 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([ 0.0052, -0.0297], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "tensor(31.1257, device='cuda:0', grad_fn=<DistCrossEntropyFuncBackward>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ada_act = AdaAct(m=0.4, h=0.333, s=64., t_alpha=1.0)\n",
    "# ada_act = AdaActOrigin(m=0.4, h=0.333, s=64., t_alpha=1.0)\n",
    "ada_fc = AdaPartialFC(margin_loss=ada_act, embedding_size=512, num_classes=10)\n",
    "ada_fc.cuda()\n",
    "loss = ada_fc(embs.cuda(), norm.cuda(), labels.cuda())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MxFaceDataset to remove failed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thucth/.conda/envs/torch19/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from dataset import MXFaceDataset\n",
    "import mxnet as mx\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/share/team/thucth/data/FaceReg/faces_emore\"\n",
    "\n",
    "path_imgrec_new = os.path.join(root_dir, 'train_new.rec')\n",
    "path_imgidx_new = os.path.join(root_dir, 'train_new.idx')\n",
    "imgrec_new = mx.recordio.MXIndexedRecordIO(path_imgidx_new, path_imgrec_new, 'w')\n",
    "\n",
    "path_imgrec = os.path.join(root_dir, 'train.rec')\n",
    "path_imgidx = os.path.join(root_dir, 'train.idx')\n",
    "imgrec = mx.recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5822653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5822653 [00:00<?, ?it/s]/tmp/ipykernel_14772/968272659.py:21: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label = torch.tensor(label, dtype=torch.long)\n",
      " 31%|███▏      | 1821604/5822653 [42:22<1:48:36, 614.03it/s] "
     ]
    }
   ],
   "source": [
    "#extract infomation from old rec \n",
    "s = imgrec.read_idx(0)\n",
    "header, _ = mx.recordio.unpack(s)\n",
    "if header.flag > 0:\n",
    "    header0 = (int(header.label[0]), int(header.label[1]))\n",
    "    imgidx = np.array(range(1, int(header.label[0])))\n",
    "else:\n",
    "    imgidx = np.array(list(imgrec.keys))\n",
    "print(len(imgidx))\n",
    "#write header record\n",
    "imgrec_new.write_idx(0,s)\n",
    "\n",
    "#write the rest records, if which record is broken, ignore it\n",
    "idx_new=1\n",
    "failed_idx=[]\n",
    "for idx in tqdm(imgidx):\n",
    "    try:\n",
    "        s = imgrec.read_idx(idx)\n",
    "        header, img = mx.recordio.unpack(s)\n",
    "        label = header.label\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        sample = mx.image.imdecode(img).asnumpy()\n",
    "        imgrec_new.write_idx(idx_new, s)\n",
    "        idx_new+=1\n",
    "    except Exception as e:\n",
    "        failed_idx.append(idx)\n",
    "        print(idx)\n",
    "        continue\n",
    "\n",
    "imgrec.close()\n",
    "imgrec_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5822653"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed_idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch19",
   "language": "python",
   "name": "torch19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
